Literature Review
=================

Introduction
------------
This coursework aims to provide a literature review of the visual analytics field,
with a focus on visualisations which describe geographical areas. It starts with a
general overview of the filed, as a brief introduction. The following sections
explore various aspects of visual analytics, both from a theoretical perspective
as well as from a practical standpoint - investigating how the research might be
applied to the project under development.

The second main section provides the theoretical background which describes the
project itself and the development process. Starting with requirements
gathering and analysis, it continues with a section describing the high-level
architecture, the employed design patterns and the system's division into components.
The following sections describe the infrastructure supporting the development process -
the version control system, build system and how is unit testing implemented.
The last section describes the first prototype - what challenges were encountered,
and how was the architecture altered to accommodate changing requirements.

Visual Analytics
----------------
Visual analytics is defined as the science of analytical reasoning facilitated
by interactive visual interfaces (Thomas 2005). It enables users to
synthesize useful information and extract insight from enormous amounts of data
which is often ambiguous and even conflicting.  

=== Analytical Reasoning
The process by which one arrives at conclusions by analyzing the data has been
described as a multi-phase process, which starts from the data and
ends with acquiring wisdom (Ackoff 1989). According to the model proposed by Ackoff, one
processes data into information by answering to the 'who', 'what', 'where' and
'when' questions.  Information can be further processed into knowledge by
answering the 'how' question. Finally, one can arrive to wisdom through an
understanding of 'why'.

[caption="Figure 1"]
image::img/png/ackoff.png[]

=== The Information Explosion
The type type of tools and techniques used during this sense-making process
vary, according to the dataset under investigation. Deriving insight from
large, dynamic datasets requires a high-level, conceptual understanding of the
data. Confronted with the information explosion which accompanied the arrival
of the information age, analysts have to cope with ever increasing quantities
of data.

According to a study by Lyman and Varian, 5 exabytes (5 x 10^18^ bytes) of new
stored information in the form of paper, film, and electronic media were
produced in 2002 alone. The amount of stored information grew at a rate of 30%
between 1999 and 2002. The study also estimates that another 18 exabytes of streaming
information flowed through electronic channels in 2002 (Lyman 2003).


==== Types of information
TODO: 
sensors, etc

Classified:

Considering these facts, it becomes obvious that analyzing such datasets is a
task which is difficult to accomplish without introducing abstraction layers.

=== Analyzing the datasets
But having the ability to accumulate large amounts of data is only one stage in
the long process of acquiring knowledge. Scientists have devised various methods
of extracting insight from the gathered data, with a varying degree of automation.

- knowledge discovery in databases ( KDD ): involves 'Data Mining' (DM) which is
defined as "a step in the KDD process concerned with the algorithmic means by which patterns or models (structures)
are enumerated from the data under acceptable computational efficiency limitations"
 
- visual data mining ( VDM )

(Icke, 2009)

=== Information Visualization
One of the most efficient methods of approaching the task of analyzing such
datasets, is employing some form of visualization. Such visualizations come in
different shapes and forms, the most common being maps, diagrams and graphs.
These methods are not a recent development - the field of 'data visualization'
has existed long before the information age; a detailed account of it's history
until recent times is given in (Friendly, 2006).

The area of data visualisation has existed since the 1800's (Icke, 2009), but
has been popularized by Edward Tufte's seminal 1990 book, "Envisioning Information"
and his later publications. In (Tufte, 1990), the author identifies and dissects,
in a scholarly manner - few words, dense with meaning and thoroughly referenced -
issues which are just as relevant today, as when the book was published.

Tufte identifies many of the principles which underpin good data visualisation,
and which one would do best to keep in mind while developing a system intended
for such use cases. Of course, it is difficult to define exactly what constitutes
a 'good visualisation', since the insight derived from a particular visualisation
may vary depending on the psychological traits of the individual performing the
analysis. 

Indeed, this fact is recognized and embraced (Thomas, 2005); often the
available information has such a dynamic and complex nature, that it cannot be
effectively tackled by individual analysts, working in isolation (p.60). This
is requirement is known as the need for 'human scalability' - a system aiming
to provide for efficient analysis of large, complex and dynamic datasets should
ensure that users will be able to share information, perform cooperative
analysis and synthesis.

However, the principles and guidelines outlined by Tufte provide demonstrable
improvements, and prevent common mistakes - which he illustrates with real-life
examples. According to Tufte, when designing a visualisations, one should strive
to "escape flatland" - find ways to convey more than the two dimensions normally
available on paper and screens. 

.Visualisations types
- describing data
- viewing relationships: between observations (network diagram), and variables (scatterplots)
- picturing data
    a. Chernoff Faces
    b. mathematical Shapes: star glyphs, superquadric shapes
    c. daisy maps
    d. heat maps
    e. tag clouds 
- temporal visualisation
- spatial visualisation
    a. natural layout: geo-spatial map
    b. abstract layout: artificial landscape map
- spatial-temporal visualisation

=== Visual Analytics
There are three major goals of visualization, namely a) presentation, b)
confirmatory analysis, and c) exploratory analysis. (Keim, 2008, p.3)

In the pursuit of wisdom, analysts use visual analytics tools. In this manner, it
becomes possible to analyze the data, formulate hypotheses and uncover patterns
that would be very difficult to detect without a visual aid. Such specialized
tools maximize the human capacity for analytical reasoning, making this process
more efficient. This combines the strength of computational brute force
available in today's machines with the human intelligence, creativity, capacity
for reasoning and analysis.

Aside from allowing analysts to focus their full cognitive and perceptual
abilities on the analytical process, a visual analytics tool should allow the
users to perform mathematical and statistical computations. Statistics are
crucial in analyzing large datasets - as is often the case with visual
analytics.

ex: supernova

==== Visual Analytics Process
interactive:
Keith 2006:
Analyse First -
Show the Important -
Zoom, Filter and Analyse Further -
Details on Demand

TODO: add an image based on (Keim, 2008 Fig.2)

color mapping 


=== Geo-spatial Visualization
TODO: add information specific to geo-spatial visualizations

Requirements Gathering and Analysis
-----------------------------------
As described in the previous section, visual analytics is a very broad
topic, encompassing numerous sub-fields. Eliciting clear, concise requirements
in this context is no easy task. After carefully considering the available
options and consulting with my supervisor, the following requirements have been
identified:

- the system should be able to load datasets - in the form of XML files
- display data on a geo-spatial heat map
- provide a timeline-based views for datasets which have time information
- allow the user to select visualisation parameters - which area to view, parameters to
  display, zoom level, the period of time to display
- provide additional visualisation methods: graph, scatterplot, star glyphs, pie charts
- perform statistical computations: mean, standard deviation

These requirements provide the general direction of the project, but they are not
set in stone and may be adjusted to accommodate the evolution of the project. Some
features might prove too difficult to implement and be dropped, or might have a
low return on investment - in other words, implementing it would be too costly and
the value it provides does not justify the effort. Other features might be added,
if they would add value to the project.

Architecture
------------
This project was designed following the 'MVC' ( Model View Controller )
paradigm. The 'model' is at the core of the application; it maintains state
information and manages the loaded data. The 'controller' is responsible for
receiving user input, in the form of mouse and keyboard events; it notifies the
model and any active views of what actions need to be performed in response. A
'view' provides the visualisation of a dataset - for example, the GDP of all
the countries in the world. Multiple views can be active simultaneously.

.Main components:
- vector graphics - parsing and rendering basic primitives
- views ( different ways of viewing 
- downloading data ( ? )
- query module - allows users to specify custom queries

Infrastructure
--------------
This section discusses how the first prototype was built. The project is laid
out following a fairly standard folder structure, encountered in many C++
projects - the source files are placed inside the `src` folder, with the header
files in `include`.  The `external` folder contains packages which are not part
of the project, but are required - at the moment, this folder contains the
`freeglut` package.

=== Version control
git

=== Build system
waf, make, vs

=== Unit Testing
The project is intended to implement 'unit testing' from the start, following a
'Test Driven Development' process (TDD). It is not a new concept, being used
since the 90's and introduced to the mainstream by Kent Beck in 2002 (Beck
2002). It has been adopted by the Agile community, and is generally recognized
as software development best practice. ( McConnell 2004, Chapter 22 ).

But due to the inherent limitations of compiled languages, TDD turns out to be
problematic area for C\+\+. A number of unit testing frameworks exist for C\+\+,
which add to the complexity since one has to decide which one to use. The
http://www.boost.org/[Boost libraries], which provide high-quality,
peer-reviewed C++ libraries, also include a unit testing framework,
http://www.boost.org/libs/test[boost test]. It was deemed appropriate since the
boost libraries will be used anyway for regular expression support - therefore
avoiding the need of installing additional libraries.

First prototype
---------------
prototype

=== Milestones
1.0

References
----------
- Thomas, J (2005) "Illuminating the path". Available from: http://nvac.pnl.gov/docs/RD_Agenda_VisualAnalytics.pdf [ accessed 26th of Jan 2012 ]
- Ackoff, R. L. (1989) "From data to wisdom". Journal of Applies Systems Analysis.
- Keim, D. A. et al (2006) "Challenges in visual data analysis". Information Visualization, IV 2006. Tenth International
- Lyman, P. et al (2003) "How Much Information?". Berkeley School of Information Management and Systems. Available from: http://www2.sims.berkeley.edu/research/projects/how-much-info-2003 [ accessed 26th of Jan 2012 ]
- Friendly, M. (2006) "Handbook of a Computational Statistics: Data Visualization", volume III. Springer-Verlag, Heidelberg.
- Icke, I. (2009) "Visual Analytics: A Multi-faceted Overview". CUNY, The Graduate Center. Available on-line from: http://web.cs.gc.cuny.edu/~iicke/academic/icke_visual_analytics_review_2009.pdf [ accessed 26th of Jan 2012 ]
- Tufte, E. (1990) "Envisioning Information". Graphics Press.
- Keim, D. A. (2008) "Visual Analytics: Scope and Challenges". Lecture Notes in Computer Science.  
- Beck, K. (2002) "Test-Driven Development: By Example". Addison-Wesley.  
- McConnell, S. (2004) "Code Complete: A Practical Handbook of Software Construction", 2nd Edition. Microsoft Press.
