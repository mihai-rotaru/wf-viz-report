Literature Review
=================

Introduction
------------
This coursework has a twofold aim; firstly, to provide a literature review of
the visual analytics field, with a focus on visualisations which describe
geographical areas.  Secondly, it provides a description of the work done so
far for the project, and the prototype which was produced.

The first part starts with an introduction to analytical reasoning, and the
'road to wisdom'. The next section discusses the type of data wich the
information age produces, and the new challenges it poses for analysts. Ways of
automated insight extraction from such datastes are described in the next
section, followed by a description of 'information visualisation', and how it
can enhance the process. This leads to the section on 'visual analytics',
followed by a more specific aspect, concerning 'geo-spatial analysis'. These
topics are explored both from a theoretical perspective as well as from a
practical standpoint - investigating how the research might be applied to the
project under development.

The second main section provides the theoretical background which describes the
project itself and the development process. Starting with requirements
gathering and analysis, it continues with a section describing the high-level
architecture, the employed design patterns and the system's division into
components.  The following sections describe the infrastructure supporting the
development process - the version control system, build system and how is unit
testing implemented. The last section describes the first prototype - what
challenges were encountered, and how was the architecture altered to
accommodate changing requirements.

Visual Analytics
----------------
Visual analytics is defined as the science of analytical reasoning facilitated
by interactive visual interfaces (Thomas 2005). It enables users to
synthesize useful information and extract insight from enormous amounts of data
which is often ambiguous and even conflicting.  

=== Analytical Reasoning
The process by which one arrives at conclusions by analyzing the data has been
described as a multi-phase process, which starts from the data and
ends with acquiring wisdom (Ackoff 1989). According to the model proposed by Ackoff, one
processes data into information by answering to the 'who', 'what', 'where' and
'when' questions.  Information can be further processed into knowledge by
answering the 'how' question. Finally, one can arrive to wisdom through an
understanding of 'why'.

[caption="Figure 1"]
image::img/png/ackoff.png[]

=== The Information Explosion
The type type of tools and techniques used during this sense-making process
vary, according to the dataset under investigation. Deriving insight from
large, dynamic datasets requires a high-level, conceptual understanding of the
data. Confronted with the information explosion which accompanied the arrival
of the information age, analysts have to cope with ever increasing quantities
of data.

==== Quantity
According to a study by Lyman and Varian, 5 exabytes (5 x 10^18^ bytes) of new
stored information in the form of paper, film, and electronic media were
produced in 2002 alone. The amount of stored information grew at a rate of 30%
between 1999 and 2002. The study also estimates that another 18 exabytes of streaming
information flowed through electronic channels in 2002 (Lyman 2003).

Scientists have increasingly large datasets to analyse; the quantity of data is
growing not just on aggregate, but for individual tasks as well. While
researching the data generated from a supernova simulation, Ma et al had to
work with a dataset weighting 4 terabytes (Ma et al, 2009). Enormous datasets
are also common in other fields, such as physics and astronomy; the need to
cope with such datasets exists now, and current trends indicate that it will
increase in the future. Considering the fact that methods of accumulating data
become more accessible and common, and the consequent increase in available
data, being able to cope with it is becoming increasingly important.

==== Types of information
Along with the staggering increases in 'quantity', the information age has
brought forth a similar increase in the 'types' of data which has become
available.

Numerous scientific fields require analysing such datasets.  In (Keim, 2008),
we find a detailed list of the fields which could benefit from an improvement
in the methods used to extract insight from such datasets, and the associated
challenges. The identified research areas range from scientific fields, such as
astronomy and physics, to business and personal information management.

The collected data can be classified, according to a number of criteria. It can
be either 'structured' or 'unstructured'; the most common type of structured
data being databases - such as MySQL databases, or CSV files. Many organisations
construct and maintain large databases. The need to analyse and extract useful
information from all these disparate databases might arise - it is desirable for
the analytical software to provide the tools to accomplish this.

The most common types of unstructured data are documents and text files. The
volume of such 'textual data' is ever increasing in volume, and steps have been
taken in the direction of automating the process of identifying entities, and
the relationships between them from textual data. Certain types of entities,
such as email addresses, phone numbers and dates have been identified by
software for some time, but an analytical software package should aid
researches in finding information of a higher level - detecting patterns and
relationships between them.

The ubiquity of smartphones and digital cameras has brought forth an abundance
of 'multimedia data' - images and video; another common source of video are
surveillance cameras, such as the CCTV system in the United Kingdom. Even
smaller scale surveillance systems, such as the one inside the offices of a
business, will generate a lot of data, since there are multiple cameras
recording at the same time, and the recordings are kept for a certain amount of
time.

The collected data might also differ in terms of 'reliability' and
'completness'. The software package must provide analysts with methods of
mitigating these issues - for example, by inserting estimates in place of the
missing or unreliable data. However, all of these decisions must be explicitly
confirmed by the scientists, thus avoiding imprecise or flat wrong results.

But as the quantity of available data increases dramatically, the human
capacity for analysis and reason remains largely unchanged. Although
technological advances can enable us to tap into a higher percentage of human
abilities, the rate at which this percentage increases is clearly dwarfed by
the exponential growth of data which needs to be analysed. This situation is
known as the 'information glut' (Thomas, 2005). Considering these facts, it
becomes obvious that analyzing such datasets is a task which is difficult to
accomplish without introducing abstraction layers.

=== Analyzing the datasets
But having the ability to accumulate large amounts of data is only one stage in
the long process of acquiring knowledge. Scientists have devised various methods
of extracting insight from the gathered data, with a varying degree of automation.

- knowledge discovery in databases ( KDD ): involves 'Data Mining' (DM) which is
defined as "a step in the KDD process concerned with the algorithmic means by which patterns or models (structures)
are enumerated from the data under acceptable computational efficiency limitations"
 
- visual data mining ( VDM )

(Icke, 2009)

=== Information Visualization
One of the most efficient methods of approaching the task of analyzing such
datasets, is employing some form of visualization. Such visualizations come in
different shapes and forms, the most common being maps, diagrams and graphs.
These methods are not a recent development - the field of 'data visualization'
has existed long before the information age; a detailed account of it's history
until recent times is given in (Friendly, 2006).

The area of data visualisation has existed since the 1800's (Icke, 2009), but
has been popularized by Edward Tufte's seminal 1990 book, "Envisioning Information"
and his later publications. In (Tufte, 1990), the author identifies and dissects,
in a scholarly manner - few words, dense with meaning and thoroughly referenced -
issues which are just as relevant today, as when the book was published.

Tufte identifies many of the principles which underpin good data visualisation,
and which one would do best to keep in mind while developing a system intended
for such use cases. Of course, it is difficult to define exactly what constitutes
a 'good visualisation', since the insight derived from a particular visualisation
may vary depending on the psychological traits of the individual performing the
analysis. 

Indeed, this fact is recognized and embraced (Thomas, 2005); often the
available information has such a dynamic and complex nature, that it cannot be
effectively tackled by individual analysts, working in isolation (p.60). This
is requirement is known as the need for 'human scalability' - a system aiming
to provide for efficient analysis of large, complex and dynamic datasets should
ensure that users will be able to share information, perform cooperative
analysis and synthesis.

However, the principles and guidelines outlined by Tufte provide demonstrable
improvements, and prevent common mistakes - which he illustrates with real-life
examples. According to Tufte, when designing a visualisations, one should strive
to "escape flatland" - find ways to convey more than the two dimensions normally
available on paper and screens. 

.Visualisations types
- describing data
- viewing relationships: between observations (network diagram), and variables (scatterplots)
- picturing data
    a. Chernoff Faces
    b. mathematical Shapes: star glyphs, superquadric shapes
    c. daisy maps
    d. heat maps
    e. tag clouds 
- temporal visualisation
- spatial visualisation
    a. natural layout: geo-spatial map
    b. abstract layout: artificial landscape map
- spatial-temporal visualisation

=== Visual Analytics
There are three major goals of visualization, namely a) presentation, b)
confirmatory analysis, and c) exploratory analysis. (Keim, 2008, p.3)

In the pursuit of wisdom, analysts use visual analytics tools. In this manner, it
becomes possible to analyze the data, formulate hypotheses and uncover patterns
that would be very difficult to detect without a visual aid. Such specialized
tools maximize the human capacity for analytical reasoning, making this process
more efficient. This combines the strength of computational brute force
available in today's machines with the human intelligence, creativity, capacity
for reasoning and analysis.

Aside from allowing analysts to focus their full cognitive and perceptual
abilities on the analytical process, a visual analytics tool should allow the
users to perform mathematical and statistical computations. Statistics are
crucial in analyzing large datasets - as is often the case with visual
analytics.

==== Visual Analytics Process
interactive:
Keith 2006:
Analyse First -
Show the Important -
Zoom, Filter and Analyse Further -
Details on Demand

TODO: add an image based on (Keim, 2008 Fig.2)

color mapping 


=== Geo-spatial Visualization
TODO: add information specific to geo-spatial visualizations

Requirements Gathering and Analysis
-----------------------------------
As described in the previous section, visual analytics is a very broad
topic, encompassing numerous sub-fields. Eliciting clear, concise requirements
in this context is no easy task. After carefully considering the available
options and consulting with my supervisor, the following requirements have been
identified:

- the system should be able to load datasets - in the form of XML files
- display data on a geo-spatial heat map
- provide a timeline-based views for datasets which have time information
- allow the user to select visualisation parameters - which area to view, parameters to
  display, zoom level, the period of time to display
- provide additional visualisation methods: graph, scatterplot, star glyphs, pie charts
- perform statistical computations: mean, standard deviation

These requirements provide the general direction of the project, but they are not
set in stone and may be adjusted to accommodate the evolution of the project. Some
features might prove too difficult to implement and be dropped, or might have a
low return on investment - in other words, implementing it would be too costly and
the value it provides does not justify the effort. Other features might be added,
if they would add value to the project.

Architecture
------------
This project was designed following the 'MVC' ( Model View Controller )
paradigm. The 'model' is at the core of the application; it maintains state
information and manages the loaded data. The 'controller' is responsible for
receiving user input, in the form of mouse and keyboard events; it notifies the
model and any active views of what actions need to be performed in response. A
'view' provides the visualisation of a dataset - for example, the GDP of all
the countries in the world. Multiple views can be active simultaneously.

.Main components:
- vector graphics - parsing and rendering basic primitives
- views ( different ways of viewing 
- downloading data ( ? )
- query module - allows users to specify custom queries

Infrastructure
--------------
This section discusses how the first prototype was built. The project is laid
out following a fairly standard folder structure, encountered in many C++
projects - the source files are placed inside the `src` folder, with the header
files in `include`.  The `external` folder contains packages which are not part
of the project, but are required - at the moment, this folder contains the
`freeglut` package.

=== Version control
git

=== Build system
waf, make, vs

=== Unit Testing
The project is intended to implement 'unit testing' from the start, following a
'Test Driven Development' process (TDD). It is not a new concept, being used
since the 90's and introduced to the mainstream by Kent Beck in 2002 (Beck
2002). It has been adopted by the Agile community, and is generally recognized
as software development best practice. ( McConnell 2004, Chapter 22 ).

But due to the inherent limitations of compiled languages, TDD turns out to be
problematic area for C\+\+. A number of unit testing frameworks exist for C\+\+,
which add to the complexity since one has to decide which one to use. The
http://www.boost.org/[Boost libraries], which provide high-quality,
peer-reviewed C++ libraries, also include a unit testing framework,
http://www.boost.org/libs/test[boost test]. It was deemed appropriate since the
boost libraries will be used anyway for regular expression support - therefore
avoiding the need of installing additional libraries.

First prototype
---------------
prototype

=== Milestones
1.0

References
----------
- Thomas, J (2005) "Illuminating the path". Available from: http://nvac.pnl.gov/docs/RD_Agenda_VisualAnalytics.pdf [ accessed 26th of Jan 2012 ]
- Ackoff, R. L. (1989) "From data to wisdom". Journal of Applies Systems Analysis.
- Keim, D. A. et al (2006) "Challenges in visual data analysis". Information Visualization, IV 2006. Tenth International
- Lyman, P. et al (2003) "How Much Information?". Berkeley School of Information Management and Systems. Available from: http://www2.sims.berkeley.edu/research/projects/how-much-info-2003 [ accessed 26th of Jan 2012 ]
- Friendly, M. (2006) "Handbook of a Computational Statistics: Data Visualization", volume III. Springer-Verlag, Heidelberg.
- Ma, K.-L. et al (2005) "Scientific Discovery through Advanced Visualization"Journal of Physics Conference Series, 2005.
- Icke, I. (2009) "Visual Analytics: A Multi-faceted Overview". CUNY, The Graduate Center. Available on-line from: http://web.cs.gc.cuny.edu/~iicke/academic/icke_visual_analytics_review_2009.pdf [ accessed 26th of Jan 2012 ]
- Tufte, E. (1990) "Envisioning Information". Graphics Press.
- Keim, D. A. (2008) "Visual Analytics: Scope and Challenges". Lecture Notes in Computer Science.  
- Beck, K. (2002) "Test-Driven Development: By Example". Addison-Wesley.  
- McConnell, S. (2004) "Code Complete: A Practical Handbook of Software Construction", 2nd Edition. Microsoft Press.
