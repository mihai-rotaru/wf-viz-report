Literature Review
=================

Abstract
--------
This report comprises the work that went into the research, design and
development of the author's third year project - including a literature
review for the field of the application - visual analytics.

The topic was chosen after studying it for another module and becoming
interested in it. One of the primary goals was to learn about and adopt best
practices used throughout the industry. This lead to using a version control
system and a bug tracker from the start. Agile methods were also considered,
but proved difficult to follow on a school/work schedule.

The goal of this project was to create an application that would enable users
to browse and analyse statistics about countries, such as the GDP, population
and national debt. It would need to provide users with a way of interacting
with the data, for example to view how it changes over time.

The application was designed to be portable, and run on Windows and Linux.
This affected the design and development process is many ways, as described in
more detail Chapter 4. One of the salient consequences was the choice
of the programming language - C++ - and development environment (Msys/MinGW).

After the literature review was completed and requirements established, the
development process started. It did not always proceed as planned. Some of the
initial requirements had to be changed, and some components were implemented in
a different manner than initially intended. Unpredicted difficulties lead to
major changes and even elimination of some components. These issues are detailed
in Chapter 5.

As a conclusion, I have to agree with Donald Knuth, "software is hard" (Knuth
2008). One of the most egregious features of software development is how
difficult it is to estimate how long it will take to implement a feature,
and that difficult to eliminate bugs can appear where one expects them the
least.

Chapter 1: Introduction
-----------------------
This coursework has a twofold aim; firstly, to provide a literature review of
the visual analytics field, with a focus on visualisations which describe
geographical areas.  Secondly, it provides a description of the work done for
the project, including an overview of the architecture and the development
process.

The first part starts with an introduction to analytical reasoning, and the
'road to wisdom'. The next section discusses the type of data which the
information age produces, and the new challenges it poses for analysts. Ways of
automated insight extraction from such datasets are described in the next
section, followed by a description of 'information visualisation', and how it
can enhance the process. This leads to the section on 'visual analytics',
followed by a more specific aspect, concerning 'geo-spatial analysis'. These
topics are explored both from a theoretical perspective as well as from a
practical standpoint - investigating how the research might be applied to the
project under development.

The second main section provides the theoretical background which describes the
project itself and the development process. Starting with requirements
gathering and analysis, it continues with a section describing the high-level
architecture, the employed design patterns and the system's division into
components and provides the justification for major architecture and design
decisions.

The following sections describe the infrastructure supporting the development
process - the version control system, build system and how is unit testing
implemented. The third section describes the first prototype - what challenges
were encountered, and how was the architecture altered to accommodate changing
requirements.


Chapter 2: Literature Review
----------------------------
Visual analytics is defined as the science of analytical reasoning facilitated
by interactive visual interfaces (Thomas 2005). It enables users to
synthesize useful information and extract insight from enormous amounts of data
which is often ambiguous and even conflicting.  

=== Analytical Reasoning
The process by which one arrives at conclusions by analyzing the data has been
described as a multi-phase process, which starts from the data and
ends with acquiring wisdom (Ackoff 1989). According to the model proposed by Ackoff, one
processes data into information by answering to the 'who', 'what', 'where' and
'when' questions.  Information can be further processed into knowledge by
answering the 'how' question. Finally, one can arrive to wisdom through an
understanding of 'why'.

.Ackoff's model
image::img/png/ackoff.png[]

=== The Information Explosion
The type type of tools and techniques used during this sense-making process
vary, according to the dataset under investigation. Deriving insight from
large, dynamic datasets requires a high-level, conceptual understanding of the
data. Confronted with the information explosion which accompanied the arrival
of the information age, analysts have to cope with ever increasing quantities
of data.

==== Quantity
According to a study by Lyman and Varian, 5 exabytes (5 x 10^18^ bytes) of new
stored information in the form of paper, film, and electronic media were
produced in 2002 alone. The amount of stored information grew at a rate of 30%
between 1999 and 2002. The study also estimates that another 18 exabytes of streaming
information flowed through electronic channels in 2002 (Lyman 2003).

Scientists have increasingly large datasets to analyse; the quantity of data is
growing not just on aggregate, but for individual tasks as well. While
researching the data generated from a supernova simulation, Ma et al had to
work with a dataset weighting 4 terabytes (Ma et al, 2009). Enormous datasets
are also common in other fields, such as physics and astronomy; the need to
cope with such datasets exists now, and current trends indicate that it will
increase in the future. Considering the fact that methods of accumulating data
become more accessible and common, and the consequent increase in available
data, being able to cope with it is becoming increasingly important.

==== Types of information
Along with the staggering increases in 'quantity', the information age has
brought forth a similar increase in the 'types' of data which has become
available.

Numerous scientific fields require analysing such datasets.  In (Keim, 2008),
we find a detailed list of the fields which could benefit from an improvement
in the methods used to extract insight from such datasets, and the associated
challenges. The identified research areas range from scientific fields, such as
astronomy and physics, to business and personal information management.

The collected data can be classified, according to a number of criteria. It can
be either 'structured' or 'unstructured'; the most common type of structured
data being databases - such as MySQL databases, or CSV files. Many organisations
construct and maintain large databases. The need to analyse and extract useful
information from all these disparate databases might arise - it is desirable for
the analytical software to provide the tools to accomplish this.

The most common types of unstructured data are documents and text files. The
volume of such 'textual data' is ever increasing in volume, and steps have been
taken in the direction of automating the process of identifying entities, and
the relationships between them from textual data. Certain types of entities,
such as email addresses, phone numbers and dates have been identified by
software for some time, but an analytical software package should aid
researches in finding information of a higher level - detecting patterns and
relationships between them.

The ubiquity of smartphones and digital cameras has brought forth an abundance
of 'multimedia data' - images and video; another common source of video are
surveillance cameras, such as the CCTV system in the United Kingdom. Even
smaller scale surveillance systems, such as the one inside the offices of a
business, will generate a lot of data, since there are multiple cameras
recording at the same time, and the recordings are kept for a certain amount of
time.

The collected data might also differ in terms of 'reliability' and
'completness' (Keim, 2008). The software package must provide analysts with methods of
mitigating these issues - for example, by inserting estimates in place of the
missing or unreliable data. However, all of these decisions must be explicitly
confirmed by the scientists, thus avoiding imprecise or flat wrong results.

But as the quantity of available data increases dramatically, the human
capacity for analysis and reason remains largely unchanged. Although
technological advances can enable us to tap into a higher percentage of human
abilities, the rate at which this percentage increases is clearly dwarfed by
the exponential growth of data which needs to be analysed. This situation is
known as the 'information glut' (Thomas, 2005). Considering these facts, it
becomes obvious that analyzing such datasets is a task which is difficult to
accomplish without introducing abstraction layers.

=== Analyzing the datasets
Having the ability to accumulate large amounts of data is only one stage in the
long process of acquiring knowledge. Scientists have devised various methods of
extracting insight from the gathered data, with a varying degree of automation.

One of the earliest developments in this field consists of 'knowledge discovery
in databases (KDD)'. This concept is discussed in (Fayyad, 1996); it involves
'Data Mining (DM)' as one of the most important steps, defined as "a step in
the KDD process concerned with the algorithmic means by which patterns or
models (structures) are enumerated from the data under acceptable computational
efficiency limitations" ( Idem, p.5).

KDD includes a number of other steps, besides DM, such as data preparation
(pre-processing), data selection, data cleaning, incorporation of appropriate
prior knowledge, and proper interpretation of the results of mining (Fayyad
1996, p.3). KDD is defined as "the nontrivial process of identifying
valid, novel, potentially useful, and ultimately understandable patterns in
data" (Idem, p.5).

But KDD suffers due to the high degree of automation and focus on mathematical
models - one of the most important challenges being the generation of intuitive
and accessible ways of conveying information to the users (Icke, 2009).
 
=== Information Visualization
One of the most efficient methods of approaching the task of analyzing such
datasets, is employing some form of visualization. The concept of 'Visual data
mining (VDM)' was introduced in the early 2000's, in recognition to the
potential which lies in the human ability to recognise patterns and make sense
of datasets which are presented visually (Keim, 2002).

Such visualizations come in different shapes and forms, the most common being
maps, diagrams and graphs.  These methods are not a recent development - the
field of 'data visualization' has existed long before the information age; a
detailed account of it's history until recent times is given in (Friendly,
2006).

The area of data visualisation has existed since the 1800's (Icke, 2009), but
has been popularized by Edward Tufte's seminal 1990 book, "Envisioning
Information" and his later publications. In (Tufte, 1990), the author
identifies and dissects, in a scholarly manner - few words, dense with meaning
and thoroughly referenced - issues which are just as relevant today, as when
the book was published.

Tufte identifies many of the principles which underpin good data visualisation,
and which one would do best to keep in mind while developing a system intended
for such use cases. Of course, it is difficult to define exactly what
constitutes a 'good visualisation', since the insight derived from a particular
visualisation may vary depending on the psychological traits of the individual
performing the analysis. 

Indeed, this fact is recognized and embraced (Thomas, 2005); often the
available information has such a dynamic and complex nature, that it cannot be
effectively tackled by individual analysts, working in isolation (p.60). This
is requirement is known as the need for 'human scalability' - a system aiming
to provide for efficient analysis of large, complex and dynamic datasets should
ensure that users will be able to share information, perform cooperative
analysis and synthesis.

However, the principles and guidelines outlined by Tufte provide demonstrable
improvements, and prevent common mistakes - which he illustrates with real-life
examples. According to Tufte, when designing a visualisations, one should
strive to "escape flatland" - find ways to convey more than the two dimensions
normally available on paper and screens, since data is often multi-dimensional
(Tufte 1990,p.13).

.Visualisations types
Numerous types of visualisations have been developed over the time.
Visualisations can be classified by the goal which the analysts hope to
achieve. One such type is represented by 'descriptive' visualisations, the
primary goal being to describe the data. This can be achieved by performing
statistical computations (e.g. mean, median) to identify the distribution of
the data (for example, normal distribution). The most common visualisations in
this context are 'bar charts', 'pie charts' and 'boxplots'. (Icke 2009, p.10)

.Example of a boxplot
image::img/png/boxplot.png[]

Often it is 'viewing relationships' that is the primary goal of the
visualisation. One of the most efficient tools for providing a high-level
overview of the relationships 'between observations' is the 'network diagram'.
The other type of relationships is 'between variables'; one of the
visualisations used for this purpose is the 'scatterplot' - which are
especially useful when the dataset has multiple dimensions, by using multiple
'scatterplots' in parallel.

.Example of a scatterplot
image::img/png/scatterplot.png[]

Numerous visualisations are available for 'picturing data'. Such methods aim to
draw on the human capacity to discern patterns in visual data. One of the most
famous data picturing method was introduced by Herman Chernoff in 1973, called
'Chernoff faces'. Faces are used because humans have a natural ability for
distinguishing faces, although the mechanism by which this happens is not yet
well understood (Icke 2009, p.11). 'Mathematical shapes' can also be used, such
as 'star glyphs'.  Other more intuitive techniques include 'heat maps' and 'tag
clouds'.

.Example of a heat map
image::img/png/heat-map.png[]

'Temporal visualisation' is usually achieved by using 'line graphs' - a simple
and intuitive form of visualisation. But when more than two dimensions are
involved, visualisations which are easily understandable become more difficult
to achieve. One method of visualising such data is by combining line graphs
with a star glyph - this method is most efficient when the graphs and the glyph
are animated; this makes the otherwise difficult to grasp information much more
accessible. A similar technique is discussed in (Tufte 1990, p.78):
"simultaneous two-dimensional indexing of the multiplied image, flatland within
flatland, significantly deepens displays, with little complication in reading".
The method described by Tufte can also be successfully employed to represent
spatial-temporal data.

.Line graph and animation view
image::img/png/time-space-glyph.png[]

The most natural way of achieving 'spatial visualisation' is by using maps,
which can convey information by using layers on top of the map. The data can be
represented either on a 'natural layout' (geo-spatial map), or on an 'abstract
layout' - when the emphasis is on attributes other than precise location in
space, and those attributes can be better illustrated by abstracting the
location. One example of such visualisations are tube maps, or circuit
diagrams.

=== Visual Analytics
There are three major goals of visualization, namely presentation, confirmatory
analysis, and exploratory analysis. (Keim, 2008, p.3). Although visualisation
methods have been developed and employed in analysis methods such as KDD and
VDM, the field of 'visual analytics' emerged as it was recognised that the
human-computer synergy can be taken a step further.

Work in the field of visual analytics started in earnest after the tragic
events of 9/11, with the seminal work of Thomas et al which resulted in
"Illuminating the Path: The Research and Development Agenda for Visual
Analytics". The authors provide an overview of the challenges faced by data
analysts, and highlight the aspects which make necessary a closer collaboration
between computers and humans in order to cope with them. These aspects include
the already discussed large datasets, with varying degrees of reliability and
completeness, but a new element is introduced: 'urgency'.

Thomas et al start off their work by listing a number of scenarios which do not
only require extracting insight from huge, heterogeneous datasets, but time
becomes of utmost importance - in emergency situations such as terrorist attacks -
the main focus of (Thomas, 2005) - it is very important to act swiftly. Therefore,
it becomes very important to optimise the analytic process.

In the pursuit of wisdom, analysts employ visual analytics software. In this
manner, it becomes possible to analyze the data, formulate hypotheses and
uncover patterns that would be very difficult to detect without a visual aid.
Such specialized tools maximize the human capacity for analytical reasoning,
making this process more efficient. This combines the strength of computational
brute force available in today's machines with the human intelligence,
creativity, capacity for reasoning and analysis.

Aside from allowing analysts to focus their full cognitive and perceptual
abilities on the analytical process, a visual analytics tool should allow the
users to perform mathematical and statistical computations. Statistics are
crucial in analyzing large datasets - as is often the case with visual
analytics.

==== Visual Analytics Process
One of the key differences between visual analytics and fields like KDD and VDM
is that in visual analytics there is a greater focus on 'interactivity'. Indeed,
the visual analytics process is often described as an 'iterative process', requiring
input, feedback and guidance from the analyst. 

A short and comprehensive mantra is provided in (Keim, 2006): "Analyse First -
Show the Important - Zoom, Filter and Analyse Further - Details on Demand". The
iterative and interactive nature of the process become obvious in this short
description. This process is illustrated below, based on (Keim, 2008 Fig.2):

.The visual analytics process: S: sources, V: visualisation, H: hypothesis, I: insight.
image::img/png/process.png[]


Chapter 3: Requirements Gathering and Analysis
----------------------------------------------

As described in the previous section, visual analytics is a very broad topic,
encompassing numerous sub-fields. Eliciting clear, concise requirements in this
context is no easy task. After carefully considering the available options and
consulting with the project supervisor, the following requirements have been
identified:

- the system should be able to load datasets - in the form of XML files
- display data on a geo-spatial heat map
- provide a timeline-based views for datasets which have time information
- allow the user to select visualisation parameters - which area to view, parameters to
  display, zoom level, the period of time to display
- provide additional visualisation methods: graph, scatterplot, star glyphs, pie charts
- perform statistical computations: mean, standard deviation

These requirements provide the general direction of the project, but they are not
set in stone and may be adjusted to accommodate the evolution of the project. Some
features might prove too difficult to implement and be dropped, or might have a
low return on investment - in other words, implementing it would be too costly and
the value it provides does not justify the effort. Other features might be added,
if they would add value to the project.

Chapter 4: Research And Design
-----------------------------
This project was designed following the 'MVC' ( Model View Controller )
paradigm. The 'model' is at the core of the application; it maintains state
information and manages the loaded data. The 'controller' is responsible for
receiving user input, in the form of mouse and keyboard events; it notifies the
model and any active views of what actions need to be performed in response. A
'view' provides the visualisation of a dataset - for example, the GDP of all
the countries in the world. Multiple views can be active simultaneously.
The application has four major components, each of which will be discussed in
more detail in the sections that follow.

=== The Graphical User Interface (GUI)
One of the major architectural decisions when developing an application is the
choice of a user interface system. The options for GUIs are to either use an
OS-specific GUI framework, such as the Microsoft Foundation Class library (MFC)
for Windows, or a cross-platform one - such as Qt (http://qt.nokia.com/products/).

Since this application was designed from the outset to run on Windows and Linux,
using MFC had to be ruled out. I also decided against Qt - although it is a fine
library, it is too heavy and complex for this project. Such frameworks are more
suitable for use in bigger applications, but for small projects it has too much
overhead and is too big.

The other option is to develop a GUI for OpenGL. Doing this from scratch, is
not an easy task. I looked for a library that could be used, but did not come
across anything that I would deem satisfactory. So I decided that I should create
the windowing system/user interface myself. One thing that needs to be kept in
mind is that such frameworks can become very complex, and I need to keep it
simple, so that it doesn't become too complex and prevent me from working on
the other aspects.

=== Loading Data
The program is intended to be used for analysing data - so this component is of
utmost importance. After considering the available options, I decided that XML
files are the best choice in this case. I considered SQL databases, since I have
some experience from previous years - SQLite (http://www.sqlite.org/) being my
top choice for a while, because it is very light, and has very good bindings for
C\++. I discarded options such as comma separated values, because they are too
rigid - a more future-proof and flexible options is needed.

Eventually I settled for XML files. Although I always considered XML as being
too verbose and heavy, I came to appreciate it's features after knowing it
better during a module we had this year. XML files provide the balance I was
seeking. In my case, I'm not sure in what format will the data to be used with
the application be. If I chose an option like SQLite, adapting the application
to support other formats would involve a lot of effort, even it a plug-in
architecture is used for the file I/O components.

But XML solves this issue. If the application supports XML, and has to consume
an SQLite database file, then I can easily write an SQLite script that would
generate an XML file, which can then be fed to the application. The reverse
scenario ( generating an SQLite database from XML files ) does not look this
simple. An added benefit of XML is XSLT - Extensible Stylesheet Language
Transformations.  If an XML file is not in a format that can be consumed by the
application, XSLT can be used to generate an acceptable file.

=== Displaying Data
How the user views and interacts with the data is of paramount importance in
visual analytics applications. Another property of such applications is that they
must allow the user to _interact_ with the data, not just view it.

This can be achieved by implementing a GUI element ( also know as 'control' )
that would allow the user to see a coloured map, that supports interaction
elements such as panning ( moving the map in any direction ) and zooming.

Chapter 5: Implementation
-------------------------
This section discusses how the first prototype was built. The project is laid
out following a fairly standard folder structure, encountered in many C++
projects - the source files are placed inside the `src` folder, with the header
files in `include`.  The `external` folder contains packages which are not part
of the project, but are required - at the moment, this folder contains the
`freeglut` package.

=== Version control
Version control is standard practice among professional programmers, and it was
therefore decided that it would be a good idea to put the project under version
control. Git, "the fast version control system" (http://git-scm.com) was chosen
mainly due to my familiarity and experience with it. Although the Windows
implementation suffers from performance and, to a lesser extent, usability
issues, the benefits a version control system offers were deemed as sufficient
to outweigh any potential disadvantages.

.The Git repository: viewed with gitg
image::img/png/git.png[]

The branching model followed by the project, known as git-flow, is described in
detail here: http://nvie.com/posts/a-successful-git-branching-model. To summarise,
it involves having two permanent branches: `master` - intended to contain only
released versions and hotfixes, and `develop` - which is intended to integrate
changes from 'feature branches'. A feature branch is created for each new feature;
development for that feature takes place on it's respective branch, without affecting
unrelated branches. When a sufficient number of features have been accumulated,
a 'release branch' is created - where the software is finalised, and tagged with
the new version. Finnally, it is merged into the `master` branch. A `hotfix` branch
will be created if a bug is discovered which needs an urgent fix; these branches
will be integrated into both `develop` and `master`.

=== Build system
The main build system is `waf` (http://code.google.com/p/waf), which describes itself
as "the meta build system". It does, indeed, provide powerful abstractions and can
be configured for use with almost any programming language. But what made it
attractive for this project were two main factors:

- it is written in Python, and the build script itself is also a Python script. This
is very fitting, because I am already quite familiar with Python. This also makes it
cross-platform.
- built-in C++ capabilities
- self-contained - the `waf` build system is distributed as a small Python script,
and does not have any dependencies - outside Python, of course.

A Visual Studio solution is also maintained. At the moment, this is done manually,
but this process is likely to be automated in the future, since `waf` can generate
Visual Studio solution and project files. Also maintained is a `makefile`, for
use with `make` on Unix systems, but it is likely that support for `make` will be
dropped, since `waf` can fulfill it's role.

=== Unit Testing
The project is intended to implement 'unit testing' from the start, following a
'Test Driven Development' process (TDD). It is not a new concept, being used
since the 90's and introduced to the mainstream by Kent Beck in 2002 (Beck
2002). It has been adopted by the Agile community, and is generally recognized
as software development best practice. ( McConnell 2004, Chapter 22 ).

But due to the inherent limitations of compiled languages, TDD turns out to be
problematic area for C\+\+. A number of unit testing frameworks exist for C\+\+,
which add to the complexity since one has to decide which one to use. The
http://www.boost.org/[Boost libraries], which provide high-quality,
peer-reviewed C++ libraries, also include a unit testing framework,
http://www.boost.org/libs/test[boost test]. It was deemed appropriate since the
boost libraries will be used anyway for regular expression support - therefore
avoiding the need of installing additional libraries.

First prototype
---------------
Below is a screenshot of the first prototype. It doesn't have much functionality,
but it does accomplish loading and displaying a map of the world, from vector
information stored in a *.pov file. 

.The first prototype
image::img/png/prototype.png[]

Two main issues become obvious form this screenshot: the lines are interrupted, and
that the drawn shapes are hollow. These issues have a high priority, and are scheduled
to be fixed in V0.3 and V0.4, respectively.

Chapter 6: Demonstration
------------------------

=== Installation
The software has no dependencies, and does not require the user to download or install
anything. It uses the freeglut and other libraries, but they are statically linked into
the executable to avoid such dependency issues.

The software is distributed as a zip file, containing a folder with the
application. To install it, all that the user needs to do is drag the folder on
his desktop, or to any other location, and click the executable file. There are
a few folders and files in the application's directory, but the executable
should be easy to spot and has a name of the form `wf-viz-VX.Y.exe`, where `X`
is the major version and `Y` is the minor version number.

The user will be presented with the applications main view:

.How the application looks when it is launched
image::img/screenshot.jpg[]

It contains two windows: one contains buttons and short explanations to allow
users to load shapefiles (maps) and data, and the other one actually displays
the data. The user can click "Load Shapefile" and he will be presented with a
standard Windows dialog, which asks him to select a `*.shp` or `*.shx` file.
When the user does this, the shapefile will be loaded into the second window.

.A shapefile was loaded
image::img/screenshot_1.jpg[]

The user can also load data. To do this, he must click "Load XML Data". An
important notice about these files, is that they must have the format of
files available on the World Bank website. They look like this:

-----------
<?xml version="1.0" encoding="utf-8"?>
<Root xmlns:wb="http://www.worldbank.org">
  <data>
    <record>
      <field name="Country or Area" key="GBR">United Kingdom</field>
      <field name="Item" key="NY.GDP.MKTP.CD">GDP (current US$)</field>
      <field name="Year">1960</field>
      <field name="Value">72328047042.1588</field>
    </record>
    <record>
      <field name="Country or Area" key="GBR">United Kingdom</field>
      <field name="Item" key="NY.GDP.MKTP.CD">GDP (current US$)</field>
      <field name="Year">1961</field>
      <field name="Value">76694360635.9159</field>
    </record>
-----------

If the selected file is invalid, the application will exit. If it is in the
required format, it's data will be rendered in the map window.

.A shapefile and data
image::img/screenshot_2.jpg[]

Chapter 7: Conclusion
---------------------
The resulting program has become quite large, but I did manage to keep the code
base ordered, with functionality in the correct files. Even though it is over
3500 lines of code, it is manageable. Unlike some previous projects, this code
base did not accrue too much technical debt. This is an important lesson, not
to deffer refactoring and to neglect coding style. Otherwise, the code base
will become difficult to manage.

Another lesson is that feature completion times cannot be reliably estimated.
Sometimes I solved some issues which I thought would be hard in less time than
expected. But, more often, I ran into issues in places where I had no
expectation to encounter them, and sometimes they are very difficult to solve.
This can be especially troublesome when the deadline is quickly approaching.

But the greatest benefit from developing this project is perhaps the experience
of actually developing a program. When learning programming, people usually solve
small programming problems; but programs such as this one, which have a certain
degree of complexity, have a completely different set of challenges.

One such challenge is that complexity does not increase linearly with the components
that are in the system. When adding or changing a component, one has to consider
not only the issues pertaining to that particular component, but how will it's 
interaction with the other components affect the overall system. This can lead to
subtle, but difficult to find bugs, which often are the cause for unpredicted
difficulties in developing a project.

References
----------
- Ackoff, R. L. (1989) "From data to wisdom". Journal of Applied Systems Analysis.
- Beck, K. (2002) "Test-Driven Development: By Example". Addison-Wesley.  
- Fayyad, U. et al (1996) "From Data Mining to Knowledge Discovery in Databases". AI Magazine, 1996. Available on-line from: http://www.aaai.org/aitopics/assets/PDF/AIMag17-03-2-article.pdf [ accessed 26th of Jan 2012 ]
- Friendly, M. (2006) "Handbook of a Computational Statistics: Data Visualization", volume III. Springer-Verlag, Heidelberg.
- Icke, I. (2009) "Visual Analytics: A Multi-faceted Overview". CUNY, The Graduate Center. Available on-line from: http://web.cs.gc.cuny.edu/~iicke/academic/icke_visual_analytics_review_2009.pdf [ accessed 26th of Jan 2012 ]
- Keim, D. A. et al (2006) "Challenges in visual data analysis". Information Visualization, IV 2006. Tenth International
- Keim, D. A. (2002) "Information Visualization and Visual Data Mining". IEEE Transactions on Visualization and Computer Graphics; Volume 8 Issue 1, January 2002.Available on-line from: http://nm.merz-akademie.de/~jasmin.sipahi/drittes/images/Keim2002.pdf [ accessed 26th of Jan 2012 ]
- Keim, D. A. (2008) "Visual Analytics: Scope and Challenges". Lecture Notes in Computer Science.  
- Knuth, D. (2008) "Donald Knuth: A Life's Work Interrupted". Communications of the ACM. Vol. 51, No. 8. Available on-line from: http://www.dcc.uchile.cl/~vramiro/d/knuth.pdf
- Lyman, P. et al (2003) "How Much Information?". Berkeley School of Information Management and Systems. Available from: http://www2.sims.berkeley.edu/research/projects/how-much-info-2003 [ accessed 26th of Jan 2012 ]
- Ma, K.-L. et al (2005) "Scientific Discovery through Advanced Visualization"Journal of Physics Conference Series, 2005.
- McConnell, S. (2004) "Code Complete: A Practical Handbook of Software Construction", 2nd Edition. Microsoft Press.
- Thomas, J et al (2005) "Illuminating the path". Available from: http://nvac.pnl.gov/docs/RD_Agenda_VisualAnalytics.pdf [ accessed 26th of Jan 2012 ]
- Tufte, E. (1990) "Envisioning Information". Graphics Press.
